{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykwn3Y4bLqmR"
   },
   "source": [
    "DATA COLLECTION ALGORITHMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qhMvNyXZGKpk"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the API endpoint URL\n",
    "url=\"https://jsonplaceholder.typicode.com/todos/1\"\n",
    "\n",
    "# Set any query parameters required by the API\n",
    "params={\"parameter1\":\"value1\",\"parameter2\":\"value2\"}\n",
    "\n",
    "# Send a GET request to the API endpoint with the query parameters\n",
    "response=requests.get(url,params=params)\n",
    "\n",
    "# Check if the response status code is 200 (OK)\n",
    "if response.status_code==200:\n",
    "    # Load the response JSON data as a dictionary\n",
    "    data=json.loads(response.text)\n",
    "\n",
    "    # Extract the data you need from the JSON dictionary\n",
    "    #chatbot_data = data[\"chatbot_data\"]\n",
    "\n",
    "    if \"chatbot_data\" in data:\n",
    "        chatbot_data=data[\"chatbot_data\"]\n",
    "    # handle the case when the key is not present\n",
    "    \n",
    "else:\n",
    "    # Handle the error response\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HzzMLlpO7Pi9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Varad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Varad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Varad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Varad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S What/WP is/VBZ the/DT sick/JJ leave/NN policy/NN ?/.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Define a function to perform NER on user input\n",
    "def perform_ner(input_text):\n",
    "    tokens = nltk.word_tokenize(input_text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "print(perform_ner(\"What is the sick leave policy?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wQXqEeQ27nRE"
   },
   "outputs": [],
   "source": [
    "%pip install chatterbot\n",
    "#%pip install chatbot\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "# Create a chatbot instance\n",
    "chatbot = ChatBot('HR Chatbot')\n",
    "\n",
    "# Train the chatbot on HR-related data\n",
    "trainer = ChatterBotCorpusTrainer(chatbot)\n",
    "trainer.train(\"chatterbot.corpus.english.hr\")\n",
    "\n",
    "# Define a function to handle user input and generate a response\n",
    "def get_response(input_text):\n",
    "  response = chatbot.get_response(input_text)\n",
    "  return str(response)\n",
    "\n",
    "  # Example usage\n",
    "print(get_response(\"What is the sick leave policy?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdtiAfzq8mSy",
    "outputId": "10129264-9af5-4017-e533-70a2751ab06c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries and modules\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import psycopg2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#!pip install chatbot\n",
    "try:\n",
    "  from chatbot import Chatbot\n",
    "  # Define data sources\n",
    "  hr_policy_docs = \"path/to/hr_policy_docs\"\n",
    "  external_data = \"path/to/external_data\"\n",
    "\n",
    "  # Collect and preprocess data\n",
    "  def collect_data():\n",
    "      # Read HR policy documents\n",
    "      hr_docs = pd.read_csv(hr_policy_docs)\n",
    "      hr_docs = hr_docs.drop_duplicates()\n",
    "      # Read external data\n",
    "      ext_data = pd.read_csv(external_data)\n",
    "      # Combine HR and external data\n",
    "      combined_data = pd.concat([hr_docs, ext_data], ignore_index=True)\n",
    "      # Clean and preprocess data\n",
    "      combined_data['text'] = combined_data['text'].apply(lambda x: clean_data(x))\n",
    "      combined_data['text'] = combined_data['text'].apply(lambda x: preprocess_data(x))\n",
    "      return combined_data\n",
    "\n",
    "  # Clean data\n",
    "  def clean_data(text):\n",
    "      # Perform text cleaning operations to remove noise and irrelevant information from the text\n",
    "      return cleaned_text\n",
    "\n",
    "  # Preprocess data\n",
    "  def preprocess_data(text):\n",
    "      # Perform text preprocessing operations such as tokenization, stemming, and lemmatization\n",
    "      return preprocessed_text\n",
    "\n",
    "  # Label data\n",
    "  def label_data(data):\n",
    "      # Label the data with categories, tags, or keywords that correspond to the topics covered in the HR policy documents\n",
    "      return labeled_data\n",
    "\n",
    "  # Store data\n",
    "  def store_data(data):\n",
    "      # Store the cleaned and labeled data in a database or file system that can be easily accessed by the chatbot\n",
    "      return stored_data\n",
    "\n",
    "  # Update data\n",
    "  def update_data():\n",
    "      # Continuously update the data to ensure that the chatbot has access to the most up-to-date information\n",
    "      return updated_data\n",
    "\n",
    "  # Natural language processing\n",
    "  def nlp(text):\n",
    "      # Use NLP algorithms to process natural language queries from employees and extract relevant information from the knowledge base\n",
    "      return nlp_output\n",
    "\n",
    "  # Machine learning\n",
    "  def machine_learning(nlp_output):\n",
    "      # Develop machine learning algorithms that can learn from employee feedback and continuously improve the chat system's performance\n",
    "      return ml_output\n",
    "\n",
    "  # Chatbot development\n",
    "  def chatbot():\n",
    "      # Integrate the NLP and machine learning algorithms into a chatbot framework that can interact with employees in a conversational manner\n",
    "      return chatbot_output\n",
    "\n",
    "  # Feedback system\n",
    "  def feedback_system():\n",
    "      # Develop a feedback system that allows employees to rate the chatbot's responses and provide feedback on how to improve the system\n",
    "      return feedback_output\n",
    "\n",
    "  # Integration with backend\n",
    "  def backend_integration():\n",
    "      # Integrate the chatbot with the backend system, including the PostgreSQL database and file system, \n",
    "      #so that it can retrieve relevant information to answer employee queries\n",
    "      return backend_output\n",
    "\n",
    "  # Main function to run the entire algorithm\n",
    "  def main():\n",
    "      # Collect and preprocess data\n",
    "      data = collect_data()\n",
    "      \n",
    "      # Label data\n",
    "      labeled_data = label_data(data)\n",
    "      \n",
    "      # Store data\n",
    "      stored_data = store_data(labeled_data)\n",
    "      \n",
    "      # Continuously update data\n",
    "      while True:\n",
    "          updated_data = update_data()\n",
    "          \n",
    "          # Natural language processing\n",
    "          nlp_output = nlp(updated_data)\n",
    "          \n",
    "          # Machine learning\n",
    "          ml_output = machine_learning(nlp_output)\n",
    "          \n",
    "          # Chatbot development\n",
    "          chatbot_output = chatbot(ml_output)\n",
    "          \n",
    "          # Feedback system\n",
    "          feedback_output = feedback_system()\n",
    "\n",
    "except:\n",
    "  print(\"Error in importing chatbot package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z66_Z7uoH502",
    "outputId": "e2fc2f6e-dbed-473d-ef28-add44601f256"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint URL\n",
    "url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "# Define the parameters for the API request\n",
    "params = {\n",
    "    \"q\": \"New York\",\n",
    "    \"appid\": \"your_api_key_here\",\n",
    "    \"units\": \"metric\"\n",
    "}\n",
    "\n",
    "# Send the API request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the response data as a JSON object\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the relevant data from the JSON object\n",
    "    temperature = data[\"main\"][\"temp\"]\n",
    "    humidity = data[\"main\"][\"humidity\"]\n",
    "    description = data[\"weather\"][0][\"description\"]\n",
    "\n",
    "    # Print the data\n",
    "    print(\"Temperature: {}Â°C\".format(temperature))\n",
    "    print(\"Humidity: {}%\".format(humidity))\n",
    "    print(\"Description: {}\".format(description))\n",
    "else:\n",
    "    # Handle the case where the API request was unsuccessful\n",
    "    print(\"API request failed with status code {}\".format(response.status_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvqUeE6GLiui"
   },
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQLu5oy3Lgsc",
    "outputId": "3dbc9341-c2aa-4d07-9af3-b60f18566965"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# define functions for text cleaning and preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower() # convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove extra whitespaces\n",
    "    return text.strip()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def preprocess(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpxVBWWTL59c"
   },
   "source": [
    "NATURAL LANGUAGE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F9UegoxMf5e",
    "outputId": "810ac69b-b3b1-458c-a5ba-57ab4b5b5408"
   },
   "outputs": [],
   "source": [
    "#Step 1: Importing the necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "#Step 2: Reading the input data\n",
    "try:\n",
    "  f = open('chatbot.txt', 'r', errors = 'ignore')\n",
    "  raw = f.read()\n",
    "  raw = raw.lower()\n",
    "  nltk.download('punkt')\n",
    "  nltk.download('wordnet')\n",
    "  sent_tokens = nltk.sent_tokenize(raw)\n",
    "  word_tokens = nltk.word_tokenize(raw) \n",
    "except:\n",
    "  print(\"Error Occurred! Try Again\")\n",
    "\n",
    "#Step 3: Preprocessing the input data\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#Step 4: Defining the greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"hello\", \"hey there\", \"hi there\", \"welcome\", \"hi, how can I help you?\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "#Step 5: Generating a response\n",
    "try:\n",
    "  def response(user_response):\n",
    "    bot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        bot_response = bot_response + \"I am sorry! I don't understand you\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response + sent_tokens[idx]\n",
    "        return bot_response\n",
    "except:\n",
    "  print(\"Error Occurred!Try Again\")\n",
    "\n",
    "#Step 6: Running the chatbot\n",
    "flag=True\n",
    "print(\"BOT: My name is Chatbot. I will answer your queries. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"BOT: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"BOT: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"BOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq5aSTRMMM_f"
   },
   "source": [
    "MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3Gq-uA4NiIv",
    "outputId": "86aa43d3-be15-42f3-f406-38ceb0d0eb6e"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define a function to preprocess input text\n",
    "def preprocess(text):\n",
    "    # Tokenize text into words\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Lemmatize words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Remove stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stopwords]\n",
    "    # Return preprocessed text as a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Define some initial greetings and responses\n",
    "greetings = [\"hi\", \"hello\", \"hey\", \"howdy\", \"hola\"]\n",
    "responses = [\"hello\", \"hi there\", \"hi\", \"I'm glad you're talking to me!\"]\n",
    "\n",
    "# Load the chatbot's knowledge base\n",
    "try:\n",
    "  with open('knowledge.txt', 'r') as f:\n",
    "    knowledge = f.read()\n",
    "except:\n",
    "  print(\"Error Occurred! Try Again\")\n",
    "\n",
    "# Preprocess knowledge base\n",
    "try:\n",
    "  preprocessed_knowledge = preprocess(knowledge)\n",
    "except:\n",
    "  print(\"Error Occurred!Try Again\")\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# Generate document-term matrix\n",
    "try:\n",
    "  doc_term_matrix = vectorizer.fit_transform([preprocessed_knowledge])\n",
    "except:\n",
    "  print(\"Error Occurred!Try Again\")\n",
    "\n",
    "# Define a function to generate responses\n",
    "def generate_response(user_input):\n",
    "    # Preprocess user input\n",
    "    preprocessed_input = preprocess(user_input)\n",
    "    # Add user input to document-term matrix\n",
    "    doc_term_matrix_user = vectorizer.transform([preprocessed_input])\n",
    "    # Compute cosine similarity between user input and knowledge base\n",
    "    similarity = cosine_similarity(doc_term_matrix_user, doc_term_matrix)[0]\n",
    "    # Get index of most similar response\n",
    "    idx = np.argmax(similarity)\n",
    "    # Return most similar response\n",
    "    return knowledge.split('\\n')[idx]\n",
    "\n",
    "# Start the chatbot\n",
    "print(\"Hi, I'm an AI chatbot. What's on your mind?\")\n",
    "while True:\n",
    "    user_input = input(\"> \")\n",
    "    # Check for exit command\n",
    "    if user_input.lower() == 'bye':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    # Check for greetings\n",
    "    if user_input.lower() in greetings:\n",
    "        print(random.choice(responses))\n",
    "    # Generate response\n",
    "    else:\n",
    "        print(generate_response(user_input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mad-v-BMRIP"
   },
   "source": [
    "CHAT BOT DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBZ-coBqN0B6",
    "outputId": "ce19e7b3-301e-4bd8-bc52-09091800b96e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define the possible responses for the chatbot\n",
    "greetings = [\"hello\", \"hi\", \"hey\", \"what's up\"]\n",
    "responses = {\n",
    "    \"hello\": \"Hello, how are you?\",\n",
    "    \"hi\": \"Hi there, how can I help you?\",\n",
    "    \"hey\": \"Hey, how's it going?\",\n",
    "    \"what's up\": \"Not much, what's up with you?\"\n",
    "}\n",
    "\n",
    "# Define a function to generate a response to a user input\n",
    "def get_response(user_input):\n",
    "    for word in user_input.split():\n",
    "        if word.lower() in greetings:\n",
    "            return random.choice(greetings)\n",
    "    if user_input.lower() in responses:\n",
    "        return responses[user_input.lower()]\n",
    "    else:\n",
    "        return \"I'm sorry, I don't understand what you're saying.\"\n",
    "\n",
    "# Define a loop to keep the chatbot running and responding to user input\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC_kgGv6MUmP"
   },
   "source": [
    "FEEDBACK SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhiD2NUNODDp",
    "outputId": "caa45611-9c28-4c11-c005-4091863c0de4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define a list of possible responses to user feedback\n",
    "feedback_responses = [\n",
    "    \"Thank you for your feedback, we will take it into consideration.\",\n",
    "    \"We appreciate your input, it will help us improve.\",\n",
    "    \"Thanks for letting us know, we will use your feedback to make improvements.\",\n",
    "]\n",
    "\n",
    "# Define a function to handle user feedback\n",
    "def handle_feedback(feedback):\n",
    "    response = random.choice(feedback_responses)\n",
    "    print(response)\n",
    "\n",
    "# Example usage\n",
    "feedback = input(\"Please enter your feedback: \")\n",
    "handle_feedback(feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q99PdIBZMWoh"
   },
   "source": [
    "INTEGRATION WITH BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgTa8MJ5OTy_"
   },
   "outputs": [],
   "source": [
    "!pip install chatterbot\n",
    "!pip install ChatBot\n",
    "!pip install flask\n",
    "!pip install ChatterBotCorpusTrainer\n",
    "!pip install flask chatterbot\n",
    "from flask import Flask, render_template, request\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ChatterBotCorpusTrainer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Create a new chat bot\n",
    "bot = ChatBot('MyBot')\n",
    "\n",
    "# Create a new trainer for the chat bot\n",
    "trainer = ChatterBotCorpusTrainer(bot)\n",
    "\n",
    "# Train the chat bot with the English corpus\n",
    "trainer.train(\"chatterbot.corpus.english\")\n",
    "\n",
    "# Define the home page route\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "# Define the chatbot API endpoint\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    user_text = request.args.get('msg')\n",
    "    bot_response = str(bot.get_response(user_text))\n",
    "    return bot_response\n",
    "\n",
    "# Start the server\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOBojxvdMbBX"
   },
   "source": [
    "TESTING AND DEPLOYEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEukb7GAOxxn",
    "outputId": "9567f4dc-a321-4004-d49c-0155d1554ee6"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "\n",
    "# Define responses to greetings\n",
    "greeting_responses = [\"Hello!\", \"Hi there!\", \"Greetings!\", \"Good Evening!\"]\n",
    "\n",
    "# Define responses to user input\n",
    "user_input_responses = [\"I'm sorry, I didn't understand that.\", \"Could you please rephrase that?\", \"Can you be more specific?\"]\n",
    "\n",
    "# Define function to handle user input\n",
    "def respond_to_user_input(user_input):\n",
    "    # Check if user input contains a greeting\n",
    "    if \"hello\" in user_input.lower() or \"hi\" in user_input.lower():\n",
    "        return random.choice(greeting_responses)\n",
    "    # If user input doesn't contain a greeting, respond with a generic response\n",
    "    else:\n",
    "        return random.choice(user_input_responses)\n",
    "\n",
    "# Define main function\n",
    "def main():\n",
    "    print(\"Hello! I am an AI chatbot. How can I assist you today?\")\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"User: \")\n",
    "        # Exit loop if user input is \"bye\"\n",
    "        if user_input.lower() == \"bye\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        # Respond to user input\n",
    "        chatbot_response = respond_to_user_input(user_input)\n",
    "        print(\"Chatbot: \" + chatbot_response)\n",
    "\n",
    "# Call main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsaWKhJ7QSvI"
   },
   "source": [
    "FULL AI CHAT BOT SOLUTION BY COMBINING ALL CODE WE HAVE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCrEcaDiQNCO"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install tensorflow\n",
    "import os\n",
    "# print(os.getcwd())\n",
    "# print(os.listdir())\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Reading and preprocessing the data\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_words = nltk.word_tokenize(pattern)\n",
    "       # print(tokenized_words)\n",
    "        words.extend(tokenized_words)\n",
    "        docs_x.append(tokenized_words)\n",
    "        docs_y.append(intent['tag'])\n",
    "\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w != '?']\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(labels)\n",
    "\n",
    "# Step 2: Creating training and testing data\n",
    "training_data = []\n",
    "output_data = []\n",
    "out_empty = [0] * len(labels)\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    for w in words:\n",
    "        if w in doc:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training_data.append(bag)\n",
    "    output_data.append(output_row)\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "output_data = np.array(output_data)\n",
    "\n",
    "# Step 3: Creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(training_data[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(output_data[0]), activation='softmax'))\n",
    "\n",
    "# Step 4: Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Training the model\n",
    "model.fit(training_data, output_data, epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Step 6: Saving the model\n",
    "model.save('chatbot_model.h5')\n",
    "\n",
    "# Step 7: Loading the model\n",
    "model = tf.keras.models.load_model('chatbot_model.h5')\n",
    "\n",
    "# Step 8: Defining the chatbot function\n",
    "def chatbot():\n",
    "    print(\"Welcome to the chatbot! Start talking with the bot (type 'quit' to stop):\")\n",
    "    while True:\n",
    "        user_input  = input(\"You :\")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        tokenized_words = nltk.word_tokenize(user_input)\n",
    "        tokenized_words = [lemmatizer.lemmatize(w.lower()) for w in tokenized_words]\n",
    "\n",
    "        input_data = np.array([0] * len(words))\n",
    "        for w in tokenized_words:\n",
    "            if w in words:\n",
    "                input_data[words.index(w)] = 1\n",
    "\n",
    "        results = model.predict(np.array([input_data]))\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for intent in data['intents']:\n",
    "            if intent['tag'] == tag:\n",
    "                responses = intent['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "\n",
    "# Running the chatbot function\n",
    "chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "18f2f34207b95f455b805bda8f36ba5c3096199f4ec20ce124a4e8043d265747"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
